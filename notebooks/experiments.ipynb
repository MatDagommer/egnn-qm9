{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[5, 11], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 19], pos=[5, 3], idx=[1], name='gdb_1', z=[5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "# Specify where to store the dataset\n",
    "dataset = QM9(root='../data/external/qm9/')\n",
    "\n",
    "# Access a sample (first molecule)\n",
    "data = dataset[0]\n",
    "\n",
    "# Print the information of the molecule\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # Remaining 20% for testing\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=200, shuffle=False)\n",
    "# train_loader = DataLoader(dataset, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   0,   0,  ..., 199, 199, 199])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from egnn_pytorch import EGNN\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "\n",
    "def convert_to_tensor_format(batch):\n",
    "    # Get the number of graphs (molecules) in the batch\n",
    "    num_graphs = batch.batch.max().item() + 1\n",
    "\n",
    "    # Initialize lists to store tensors for each molecule\n",
    "    feature_tensors = []\n",
    "    pos_tensors = []\n",
    "\n",
    "\n",
    "    for i in range(num_graphs):\n",
    "        # Extract the nodes corresponding to the i-th molecule\n",
    "        node_indices = (batch.batch == i).nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "        # Get the features and positions for the i-th molecule\n",
    "        features = batch.x[node_indices]  # Shape [num_nodes_i, dimension]\n",
    "        positions = batch.pos[node_indices]  # Shape [num_nodes_i, 3]\n",
    "\n",
    "        # Add batch dimension (1, num_nodes, dim)\n",
    "        feature_tensors.append(features.unsqueeze(0))  # Shape [1, num_nodes_i, dimension]\n",
    "        pos_tensors.append(positions.unsqueeze(0))     # Shape [1, num_nodes_i, 3]\n",
    "\n",
    "    # Concatenate all molecules along the batch dimension\n",
    "\n",
    "    return feature_tensors, pos_tensors\n",
    "\n",
    "# Define the MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n",
    "        self.relu = nn.ReLU()                         # ReLU activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # Second fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)   # Pass input through the first layer\n",
    "        x = self.relu(x)  # Apply ReLU activation\n",
    "        x = self.fc2(x)   # Pass through second layer\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMPredictor(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            node_dim = 11,\n",
    "            latent_size=128, \n",
    "            hidden_size=128, \n",
    "            output_size=19,\n",
    "            num_layers=3\n",
    "        ):\n",
    "        super(QMPredictor, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            EGNN(dim = node_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.pool_fn = global_mean_pool\n",
    "        self.mlp = MLP(node_dim, hidden_size, output_size)\n",
    "\n",
    "    def forward(self, feats, coors):\n",
    "        for layer in self.layers:\n",
    "            feats, coors = layer(feats, coors)\n",
    "        batch=torch.zeros([feats.size(1)], dtype=torch.int64)\n",
    "        pooled_tensor = self.pool_fn(x=feats[0], batch=batch)\n",
    "        out = self.mlp(pooled_tensor)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGNNLayer(MessagePassing):\n",
    "    def __init__(self, node_feat_dim, edge_feat_dim, coord_dim=3, aggr=\"mean\"):\n",
    "        super(EGNNLayer, self).__init__(aggr=aggr)\n",
    "        self.node_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2*node_feat_dim + edge_feat_dim + 1, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, node_feat_dim)\n",
    "        )\n",
    "        self.coord_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(node_feat_dim, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, coord):\n",
    "        # batch argument helps to know which nodes belong to which graph in the batch\n",
    "        return self.propagate(edge_index, x=x, coord=coord, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_i, x_j, coord_i, coord_j, edge_attr):\n",
    "        d_ij = coord_j - coord_i\n",
    "        distance_ij = torch.norm(d_ij, dim=-1, keepdim=True)\n",
    "\n",
    "        m_ij = torch.cat([x_i, x_j, edge_attr, distance_ij], dim=-1)\n",
    "\n",
    "        node_output = self.node_mlp(m_ij)\n",
    "        # coord_output = self.coord_mlp(x_j)\n",
    "        # print(\"coord: \", coord_output.size())\n",
    "        # print(\"node_output size: \", node_output.size())\n",
    "\n",
    "        return node_output\n",
    "\n",
    "    def update(self, aggr_out, coord):\n",
    "        # print(\"coord: \", coord.size())\n",
    "        # print(\"aggr_out[1] size: \", aggr_out[1].size())\n",
    "        node_update = aggr_out[0]\n",
    "        # coord_update = coord + aggr_out[1]\n",
    "\n",
    "        return node_update, coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatible with batches\n",
    "class QMPredictorBatch(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            node_feat_dim = 11,\n",
    "            edge_feat_dim = 4,\n",
    "            coord_dim = 3,\n",
    "            latent_size=128, \n",
    "            hidden_size=128, \n",
    "            output_size=19,\n",
    "            num_layers=3\n",
    "        ):\n",
    "        super(QMPredictorBatch, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            EGNNLayer(node_feat_dim, edge_feat_dim, coord_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.pool_fn = global_mean_pool\n",
    "        self.mlp = MLP(node_feat_dim, hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, coord, batch=None):\n",
    "        for layer in self.layers:\n",
    "            print(x.size())\n",
    "            # print(edge_index.size())\n",
    "            # print(edge_attr.size())\n",
    "            # print(coord.size())\n",
    "            # print(batch.size())\n",
    "            x, _ = layer(x, edge_index, edge_attr, coord)#, batch=batch)\n",
    "        print(x.size())\n",
    "        print(batch.size())\n",
    "        pooled_tensor = self.pool_fn(x=x, batch=batch)\n",
    "        out = self.mlp(pooled_tensor)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = QMPredictor()\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Initialize lists to store losses\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# num_epochs = 1\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     epoch_train_loss = 0\n",
    "\n",
    "#     for graphs in tqdm(train_loader, desc=\"Training\", unit=\"batch\"):\n",
    "#         feats, coors = convert_to_tensor_format(graphs)\n",
    "#         labels = graphs.y\n",
    "#         outputs = []\n",
    "\n",
    "#         for i in range(len(feats)):\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(feats[i], coors[i])\n",
    "#             outputs.append(output)\n",
    "\n",
    "#         outputs = torch.cat(outputs, dim=0)\n",
    "#         # Check if the shapes of outputs and labels match\n",
    "#         if outputs.shape != labels.shape:\n",
    "#             print(f\"Shape mismatch: outputs {outputs.shape}, labels {labels.shape}\")\n",
    "#             continue  # Skip this iteration if shapes do not match\n",
    "\n",
    "#         loss = criterion(outputs, labels) if labels is not None else None  # Handle loss calculation\n",
    "#         if loss is not None:\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             epoch_train_loss += loss.item()\n",
    "\n",
    "#      # Average training loss for the epoch\n",
    "#     train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "#     # Validation step\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     epoch_val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for graphs in test_loader:\n",
    "#             feats, coors = convert_to_tensor_format(graphs)\n",
    "#             labels = graphs.y\n",
    "#             outputs = []\n",
    "#             for i in range(len(feats)):\n",
    "#                 output = model(feats[i], coors[i])\n",
    "#                 outputs.append(output)\n",
    "\n",
    "#             outputs = torch.cat(outputs, dim=0)\n",
    "#             loss = criterion(outputs, labels) if labels is not None else None\n",
    "#             if loss is not None:\n",
    "#                 epoch_val_loss += loss.item()\n",
    "\n",
    "#     # Average validation loss for the epoch\n",
    "#     val_losses.append(epoch_val_loss / len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[7, 11], edge_index=[2, 12], edge_attr=[12, 4], y=[1, 19], pos=[7, 3], idx=[1], name='gdb_11', z=[7])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/524 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[3582, 11], edge_index=[2, 7396], edge_attr=[7396, 4], y=[200, 19], pos=[3582, 3], idx=[200], name=[200], z=[3582], batch=[3582], ptr=[201])\n",
      "tensor([  0,   0,   0,  ..., 199, 199, 199])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for graphs in tqdm(train_loader, desc=\"Training\", unit=\"batch\"):\n",
    "    graphs\n",
    "    print(graphs)\n",
    "    print(graphs.batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/524 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3622, 11])\n",
      "torch.Size([11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got -2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m coord \u001b[38;5;241m=\u001b[39m graphs\u001b[38;5;241m.\u001b[39mpos  \u001b[38;5;66;03m# Random initial coordinates\u001b[39;00m\n\u001b[1;32m     24\u001b[0m batch_index \u001b[38;5;241m=\u001b[39m graphs\u001b[38;5;241m.\u001b[39mbatch  \u001b[38;5;66;03m# Index of each node in the batch\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoord\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, batch=batch_index)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels) \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Handle loss calculation\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[126], line 28\u001b[0m, in \u001b[0;36mQMPredictorBatch.forward\u001b[0;34m(self, x, edge_index, edge_attr, coord, batch)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# print(edge_index.size())\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# print(edge_attr.size())\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# print(coord.size())\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# print(batch.size())\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoord\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, batch=batch)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[119], line 17\u001b[0m, in \u001b[0;36mEGNNLayer.forward\u001b[0;34m(self, x, edge_index, edge_attr, coord)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_attr, coord):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# batch argument helps to know which nodes belong to which graph in the batch\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoord\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoord\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:514\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m decomp_args:\n\u001b[1;32m    512\u001b[0m         kwargs[arg] \u001b[38;5;241m=\u001b[39m decomp_kwargs[arg][i]\n\u001b[0;32m--> 514\u001b[0m coll_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_user_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmutable_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m msg_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minspector\u001b[38;5;241m.\u001b[39mcollect_param_data(\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m, coll_dict)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:360\u001b[0m, in \u001b[0;36mMessagePassing._collect\u001b[0;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m     data \u001b[38;5;241m=\u001b[39m data[dim]\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor):\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lift(data, edge_index, dim)\n\u001b[1;32m    363\u001b[0m out[arg] \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:257\u001b[0m, in \u001b[0;36mMessagePassing._set_size\u001b[0;34m(self, size, dim, src)\u001b[0m\n\u001b[1;32m    255\u001b[0m the_size \u001b[38;5;241m=\u001b[39m size[dim]\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m the_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 257\u001b[0m     size[dim] \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m the_size \u001b[38;5;241m!=\u001b[39m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEncountered tensor with size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but expected size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthe_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got -2)"
     ]
    }
   ],
   "source": [
    "model = QMPredictorBatch()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    for graphs in tqdm(train_loader, desc=\"Training\", unit=\"batch\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        node_features = graphs.x\n",
    "        labels = graphs.y\n",
    "        edge_index = graphs.edge_index\n",
    "        edge_attr = graphs.edge_attr\n",
    "        coord = graphs.pos  # Random initial coordinates\n",
    "        batch_index = graphs.batch  # Index of each node in the batch\n",
    "\n",
    "        outputs = model(node_features, edge_index, edge_attr, coord)#, batch=batch_index)\n",
    "\n",
    "        loss = criterion(outputs, labels) if labels is not None else None  # Handle loss calculation\n",
    "        if loss is not None:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "     # Average training loss for the epoch\n",
    "    train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for graphs in test_loader:\n",
    "            labels = graphs.y\n",
    "            node_features = graphs.x\n",
    "            edge_index = graphs.edge_index\n",
    "            edge_attr = graphs.edge_attr\n",
    "            coord = graphs.pos  # Random initial coordinates\n",
    "            batch_index = graphs.batch  # Index of each node in the batch\n",
    "            outputs = model(node_features, edge_index, edge_attr, coord)#, batch=batch_index)\n",
    "\n",
    "            loss = criterion(outputs, labels) if labels is not None else None\n",
    "            if loss is not None:\n",
    "                epoch_val_loss += loss.item()\n",
    "\n",
    "    # Average validation loss for the epoch\n",
    "    val_losses.append(epoch_val_loss / len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98622.84306416985]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4679616.787504473]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plotting the learning curves\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Plotting the learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egnn-qm9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
