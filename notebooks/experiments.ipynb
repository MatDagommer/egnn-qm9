{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[5, 11], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 19], pos=[5, 3], idx=[1], name='gdb_1', z=[5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from egnn_pytorch import EGNN\n",
    "from torch_geometric.utils import softmax\n",
    "from loguru import logger\n",
    "\n",
    "# Specify where to store the dataset\n",
    "dataset = QM9(root='../data/external/qm9/')\n",
    "\n",
    "# Access a sample (first molecule)\n",
    "data = dataset[0]\n",
    "\n",
    "# Print the information of the molecule\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = EGNN(dim = 512, edge_dim = 4)\n",
    "layer2 = EGNN(dim = 512, edge_dim = 4)\n",
    "\n",
    "feats = torch.randn(1, 16, 512)\n",
    "coors = torch.randn(1, 16, 3)\n",
    "edges = torch.randn(1, 16, 16, 4)\n",
    "\n",
    "feats, coors = layer1(feats, coors, edges)\n",
    "feats, coors = layer2(feats, coors, edges) # (1, 16, 512), (1, 16, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # Remaining 20% for testing\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=200, shuffle=False)\n",
    "# train_loader = DataLoader(dataset, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   0,   0,  ..., 199, 199, 199])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3542, 3542)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor_format(batch):\n",
    "    # Get the number of graphs (molecules) in the batch\n",
    "    num_graphs = batch.batch.max().item() + 1\n",
    "\n",
    "    # Initialize lists to store tensors for each molecule\n",
    "    feature_tensors = []\n",
    "    pos_tensors = []\n",
    "\n",
    "    for i in range(num_graphs):\n",
    "        # Extract the nodes corresponding to the i-th molecule\n",
    "        node_indices = (batch.batch == i).nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "        # Get the features and positions for the i-th molecule\n",
    "        features = batch.x[node_indices]  # Shape [num_nodes_i, dimension]\n",
    "        positions = batch.pos[node_indices]  # Shape [num_nodes_i, 3]\n",
    "\n",
    "        # Add batch dimension (1, num_nodes, dim)\n",
    "        feature_tensors.append(features.unsqueeze(0))  # Shape [1, num_nodes_i, dimension]\n",
    "        pos_tensors.append(positions.unsqueeze(0))     # Shape [1, num_nodes_i, 3]\n",
    "\n",
    "    # Concatenate all molecules along the batch dimension\n",
    "\n",
    "    return feature_tensors, pos_tensors\n",
    "\n",
    "# Define the MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n",
    "        self.relu = nn.ReLU()                         # ReLU activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # Second fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)   # Pass input through the first layer\n",
    "        x = self.relu(x)  # Apply ReLU activation\n",
    "        x = self.fc2(x)   # Pass through second layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adjacency_tensor(edge_index, edge_attr, num_nodes):\n",
    "    # Initialize empty adjacency tensor\n",
    "    # Shape: [1, num_nodes, num_nodes, edge_feature_dim]\n",
    "    adj_tensor = torch.zeros(1, num_nodes, num_nodes, edge_attr.size(-1))\n",
    "    \n",
    "    # Get source and target nodes\n",
    "    src, dst = edge_index\n",
    "    \n",
    "    # Populate the adjacency tensor\n",
    "    # Note: we're adding batch dimension hence the 0 index\n",
    "    adj_tensor[0, src, dst] = edge_attr\n",
    "    \n",
    "    return adj_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMPredictor(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            node_dim = 11,\n",
    "            latent_size=128, \n",
    "            hidden_size=128, \n",
    "            output_size=19,\n",
    "            num_layers=3\n",
    "        ):\n",
    "        super(QMPredictor, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            # EGNN(dim = node_dim)\n",
    "            EGNN(dim = 512, edge_dim = 4)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.pool_fn = global_mean_pool\n",
    "        self.mlp = MLP(node_dim, hidden_size, output_size)\n",
    "\n",
    "    def forward(self, feats, coors, edges):\n",
    "        for layer in self.layers:\n",
    "            feats, coors = layer(feats, coors, edges)\n",
    "        batch=torch.zeros([feats.size(1)], dtype=torch.int64)\n",
    "        pooled_tensor = self.pool_fn(x=feats[0], batch=batch)\n",
    "        out = self.mlp(pooled_tensor)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EGNNLayer(MessagePassing):\n",
    "#     def __init__(self, node_feat_dim, edge_feat_dim, coord_dim=3, aggr=\"mean\"):\n",
    "#         super(EGNNLayer, self).__init__(aggr=aggr)\n",
    "#         self.node_mlp = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(2*node_feat_dim + edge_feat_dim + 1, 64),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(64, node_feat_dim)\n",
    "#         )\n",
    "#         self.coord_mlp = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(node_feat_dim, 64),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(64, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, edge_index, edge_attr, coord):\n",
    "#         # batch argument helps to know which nodes belong to which graph in the batch\n",
    "#         return self.propagate(edge_index, x=x, coord=coord, edge_attr=edge_attr)\n",
    "\n",
    "#     def message(self, x_i, x_j, coord_i, coord_j, edge_attr):\n",
    "#         d_ij = coord_j - coord_i\n",
    "#         distance_ij = torch.norm(d_ij, dim=-1, keepdim=True)\n",
    "\n",
    "#         m_ij = torch.cat([x_i, x_j, edge_attr, distance_ij], dim=-1)\n",
    "\n",
    "#         node_output = self.node_mlp(m_ij)\n",
    "#         # coord_output = self.coord_mlp(x_j)\n",
    "#         # print(\"coord: \", coord_output.size())\n",
    "#         # print(\"node_output size: \", node_output.size())\n",
    "\n",
    "#         return node_output\n",
    "\n",
    "#     def update(self, aggr_out, coord):\n",
    "#         # print(\"coord: \", coord.size())\n",
    "#         # print(\"aggr_out[1] size: \", aggr_out[1].size())\n",
    "#         node_update = aggr_out[0]\n",
    "#         # coord_update = coord + aggr_out[1]\n",
    "\n",
    "#         return node_update, coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compatible with batches\n",
    "# class QMPredictorBatch(nn.Module):\n",
    "#     def __init__(\n",
    "#             self, \n",
    "#             node_feat_dim = 11,\n",
    "#             edge_feat_dim = 4,\n",
    "#             coord_dim = 3,\n",
    "#             latent_size=128, \n",
    "#             hidden_size=128, \n",
    "#             output_size=19,\n",
    "#             num_layers=3\n",
    "#         ):\n",
    "#         super(QMPredictorBatch, self).__init__()\n",
    "#         self.layers = torch.nn.ModuleList([\n",
    "#             EGNNLayer(node_feat_dim, edge_feat_dim, coord_dim)\n",
    "#             for _ in range(num_layers)\n",
    "#         ])\n",
    "#         self.pool_fn = global_mean_pool\n",
    "#         self.mlp = MLP(node_feat_dim, hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x, edge_index, edge_attr, coord, batch=None):\n",
    "#         for layer in self.layers:\n",
    "#             print(x.size())\n",
    "#             print(edge_index.size())\n",
    "#             print(edge_attr.size())\n",
    "#             print(coord.size())\n",
    "#             print(batch.size())\n",
    "#             x, _ = layer(x, edge_index, edge_attr, coord)#, batch=batch)\n",
    "#         print(x.size())\n",
    "#         print(batch.size())\n",
    "#         pooled_tensor = self.pool_fn(x=x, batch=batch)\n",
    "#         out = self.mlp(pooled_tensor)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = QMPredictor()\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Initialize lists to store losses\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# num_epochs = 1\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     epoch_train_loss = 0\n",
    "\n",
    "#     for graphs in tqdm(train_loader, desc=\"Training\", unit=\"batch\"):\n",
    "#         feats, coors = convert_to_tensor_format(graphs)\n",
    "#         labels = graphs.y\n",
    "#         outputs = []\n",
    "\n",
    "#         for i in range(len(feats)):\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(feats[i], coors[i])\n",
    "#             outputs.append(output)\n",
    "\n",
    "#         outputs = torch.cat(outputs, dim=0)\n",
    "#         # Check if the shapes of outputs and labels match\n",
    "#         if outputs.shape != labels.shape:\n",
    "#             print(f\"Shape mismatch: outputs {outputs.shape}, labels {labels.shape}\")\n",
    "#             continue  # Skip this iteration if shapes do not match\n",
    "\n",
    "#         loss = criterion(outputs, labels) if labels is not None else None  # Handle loss calculation\n",
    "#         if loss is not None:\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             epoch_train_loss += loss.item()\n",
    "\n",
    "#      # Average training loss for the epoch\n",
    "#     train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "#     # Validation step\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     epoch_val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for graphs in test_loader:\n",
    "#             feats, coors = convert_to_tensor_format(graphs)\n",
    "#             labels = graphs.y\n",
    "#             outputs = []\n",
    "#             for i in range(len(feats)):\n",
    "#                 output = model(feats[i], coors[i])\n",
    "#                 outputs.append(output)\n",
    "\n",
    "#             outputs = torch.cat(outputs, dim=0)\n",
    "#             loss = criterion(outputs, labels) if labels is not None else None\n",
    "#             if loss is not None:\n",
    "#                 epoch_val_loss += loss.item()\n",
    "\n",
    "#     # Average validation loss for the epoch\n",
    "#     val_losses.append(epoch_val_loss / len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[7, 11], edge_index=[2, 12], edge_attr=[12, 4], y=[1, 19], pos=[7, 3], idx=[1], name='gdb_11', z=[7])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/524 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[3673, 11], edge_index=[2, 7610], edge_attr=[7610, 4], y=[200, 19], pos=[3673, 3], idx=[200], name=[200], z=[3673], batch=[3673], ptr=[201])\n",
      "tensor([  0,   0,   0,  ..., 199, 199, 199])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for graphs in tqdm(train_loader, desc=\"Training\", unit=\"batch\"):\n",
    "    graphs\n",
    "    print(graphs)\n",
    "    print(graphs.batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/524 [00:00<?, ?batch/s]\u001b[32m2024-11-07 19:09:54.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mtorch.Size([1, 3655, 11])\u001b[0m\n",
      "\u001b[32m2024-11-07 19:09:54.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mtorch.Size([1, 3655, 3])\u001b[0m\n",
      "\u001b[32m2024-11-07 19:09:54.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mtorch.Size([1, 3655, 3655, 4])\u001b[0m\n",
      "Training:   0%|          | 0/524 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (13359025x27 and 1029x2058)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 46\u001b[0m\n\u001b[1;32m     40\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(coord\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     41\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(edge_tensor\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 46\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoord\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, batch=batch_index)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels) \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Handle loss calculation\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m, in \u001b[0;36mQMPredictor.forward\u001b[0;34m(self, feats, coors, edges)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, feats, coors, edges):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 21\u001b[0m         feats, coors \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     batch\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros([feats\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m     23\u001b[0m     pooled_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_fn(x\u001b[38;5;241m=\u001b[39mfeats[\u001b[38;5;241m0\u001b[39m], batch\u001b[38;5;241m=\u001b[39mbatch)\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/egnn_pytorch/egnn_pytorch.py:287\u001b[0m, in \u001b[0;36mEGNN.forward\u001b[0;34m(self, feats, coors, edges, mask, adj_mat)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(edges):\n\u001b[1;32m    285\u001b[0m     edge_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((edge_input, edges), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 287\u001b[0m m_ij \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_gate):\n\u001b[1;32m    290\u001b[0m     m_ij \u001b[38;5;241m=\u001b[39m m_ij \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_gate(m_ij)\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/egnn-qm9/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (13359025x27 and 1029x2058)"
     ]
    }
   ],
   "source": [
    "model = QMPredictor()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    for graphs in tqdm(train_loader, desc=\"Training\", unit=\"batch\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        node_features = graphs.x\n",
    "        labels = graphs.y\n",
    "        edge_index = graphs.edge_index\n",
    "        edge_attr = graphs.edge_attr\n",
    "        coord = graphs.pos  # Random initial coordinates\n",
    "        batch_index = graphs.batch  # Index of each node in the batch\n",
    "\n",
    "        node_features = node_features.unsqueeze(0)\n",
    "        coord = coord.unsqueeze(0)\n",
    "\n",
    "        # Get number of nodes in the graph\n",
    "        num_nodes = graphs.x.size(0)\n",
    "        \n",
    "        # Build adjacency tensor\n",
    "        edge_tensor = build_adjacency_tensor(\n",
    "            graphs.edge_index, \n",
    "            graphs.edge_attr, \n",
    "            num_nodes\n",
    "        )\n",
    "\n",
    "        logger.info(node_features.size())\n",
    "        logger.info(coord.size())\n",
    "        logger.info(edge_tensor.size())\n",
    "\n",
    "        outputs = model(node_features, coord, edge_tensor)#, batch=batch_index)\n",
    "\n",
    "        loss = criterion(outputs, labels) if labels is not None else None  # Handle loss calculation\n",
    "        if loss is not None:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "     # Average training loss for the epoch\n",
    "    train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for graphs in test_loader:\n",
    "            labels = graphs.y\n",
    "            node_features = graphs.x\n",
    "            edge_index = graphs.edge_index\n",
    "            edge_attr = graphs.edge_attr\n",
    "            coord = graphs.pos  # Random initial coordinates\n",
    "            batch_index = graphs.batch  # Index of each node in the batch\n",
    "            outputs = model(node_features, edge_index, edge_attr, coord)#, batch=batch_index)\n",
    "\n",
    "            loss = criterion(outputs, labels) if labels is not None else None\n",
    "            if loss is not None:\n",
    "                epoch_val_loss += loss.item()\n",
    "\n",
    "    # Average validation loss for the epoch\n",
    "    val_losses.append(epoch_val_loss / len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egnn-qm9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
